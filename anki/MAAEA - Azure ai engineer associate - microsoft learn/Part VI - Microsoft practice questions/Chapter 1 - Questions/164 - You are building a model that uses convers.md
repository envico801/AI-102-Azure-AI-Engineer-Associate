========== Question ==========  

### You are building a model that uses Conversational Language Understanding (CLU).

You need to measure how accurate the model is by using the ratio between the correctly identified positives (true positives) and all identified positives.

Which metric should you use?

Select only one answer.

-   Bilingual Evaluation Understudy (BLEU)

-   F1 score

-   precision

-   recall  

========== Answer ==========  

---

###### 1. precision

Precision measures how precise/accurate a model is. It is the ratio between the correctly identified positives (true positives) and all identified positives. The precision metric reveals how many of the predicted classes are correctly labeled.

Recall measures the model's ability to predict actual positive classes. F1 score is a function of precision and recall. BLEU is from the Azure AI Translator service.

[Conversational Language Understanding evaluation metrics - Azure Cognitive Services | Microsoft Learn](https://learn.microsoft.com/azure/cognitive-services/language-service/conversational-language-understanding/concepts/evaluation-metrics)

[Build a Language Understanding model - Training | Microsoft Learn](https://learn.microsoft.com/training/modules/build-language-understanding-model/)

========== Id ==========  
164

---

DECK INFO

TARGET DECK: Programming::Cloud::Azure::MAAEA - Azure ai engineer associate - microsoft learn::Part VI - Microsoft practice questions::Chapter 1 - Questions

FILE TAGS: #Programming::#Cloud::#Azure::#AI-102::#MAAEA-Azure-ai-engineer-associate-microsoft-learn::#Part-VI-Microsoft-practice-questions::#Chapter-1-Questions::#164-You-are-building-a-model-that-uses-convers

Tags:

Reference:

Related:

```dataview
LIST
where file.name = this.file.name
```

QUESTION STATUS: Safe to store
